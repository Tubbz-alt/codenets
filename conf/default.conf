
lang_ids {
    php = 0
    python = 1
    ruby = 2
    java = 3
    go = 4
    javascript = 5
}

common_vocab_size = 10000

bert {
    hidden_size = 128
    vocab_size = ${common_vocab_size}
    intermediate_size = 512
    num_hidden_layers = 3
    num_attention_heads = 8
}

tokenizers {
    build_path = "./build_tokenizers"
}

dataset {
    root_dir = ${HOME}"/workspaces/tools/CodeSearchNet/resources"
    common_params {
        fraction_using_func_name=0.1
        min_len_func_name_for_query=12
        use_subtokens=False
        mark_subtoken_end=False
        code_max_num_tokens=200
        query_max_num_tokens=30
        use_bpe=True
        vocab_size=${common_vocab_size}
        pct_bpe=0.5
        vocab_count_threshold=10
        lang_ids = ${lang_ids}
        do_lowercase = true
        special_tokens = ["<unk>"]
        parallelize = true
    }

    train {
        dirs = ${dataset.root_dir}"/data_dirs_train.txt"
        params = ${dataset.common_params}
    }

    val {
        dirs = ${dataset.root_dir}"/data_dirs_valid.txt"
        params = ${dataset.common_params}
    }

    test {
        dirs = ${dataset.root_dir}"/data_dirs_test.txt"
        params = ${dataset.common_params}
    }

    queries_file = ${dataset.root_dir}"/queries.csv"
}

training {
    # name = "default"
    # iteration = "default"

    short_circuit = False
    seed = 0
    device = "cuda"
    lr = 0.0001
    max_grad_norm = 1.0
    min_log_interval = 50
    start_epoch = 0
    epochs = 10

    batch_size {
        train = 256
        val = 256
        test = 256
    }

    loss {
        type = "softmax_cross_entropy"
        margin = 1.0
    }

    pickle_path = "./pickles"
    output_dir = "./checkpoints"

    tensorboard = true
    tensorboard_path = "./runs"

    wandb = true
    
}