include "./default.conf"

common_vocab_size = 60000 # 5 lang + 1 query * 10000

tokenizers {
    build_path = "./build_tokenizers/with_lang_query_code_siamese"
    token_files = "./build_tokenizers/token_files_query_code_siamese"
}

dataset {
    common_params {
        parallelize = false
        do_lowercase = true
        special_tokens = ["<unk>", "<lg>", "<qy>"]
    }
}

training {
    short_circuit = true

    device = "cuda"
    wandb = false
    tensorboard = false

    name = "code_search_siamese"
    iteration = "2020_02_15_14_00"
    tokenizer_type = "query_code_siamese"
    # Temporary because Rust tokenizers do not manage common tokens
    common_tokens_file = "./pickles/common_tokens_"${training.tokenizer_type}"_"${iteration}".p"

    model {
        training_ctx_class = "codenets.codesearchnet.query_code_siamese.training_ctx.QueryCodeSiameseCtx"
        output_size = 72
        encoder {
            hidden_size = ${training.model.output_size}
            vocab_size = ${common_vocab_size}
            intermediate_size = 256
            num_hidden_layers = 12
            num_attention_heads = 12
        }
    }

    batch_size {
        train = 100
        val = 100
        test = 100
    }

}