include "./default.conf"

# bert {
#     hidden_size = 256
#     vocab_size = ${common_vocab_size}
#     intermediate_size = 1024
#     num_hidden_layers = 6
#     num_attention_heads = 8
# }

tokenizers {
    build_path = "./build_tokenizers/with_lang"
}

dataset {
    common_params {
        parallelize = false
        do_lowercase = true
        special_tokens = ["<unk>", "<lg>"]
    }
}

training {
    name = "code_search_bert_lg"
    iteration = "2020_02_05_00_00"

    batch_size {
        train = 170
        val = 170
        test = 170
    }

    model {
        type = "single_query_single_code"
        output_size = 128
        query_encoder {
            hidden_size = ${training.model.output_size}
            vocab_size = ${common_vocab_size}
            intermediate_size = 512
            num_hidden_layers = 3
            num_attention_heads = 8
        }
        code_encoder {
            hidden_size = ${training.model.output_size}
            vocab_size = ${common_vocab_size}
            intermediate_size = 1024
            num_hidden_layers = 6
            num_attention_heads = 8
        }
    }

}