include "./default.conf"

common_vocab_size = 30000 # 5 lang + 1 query * 5000


tokenizers {
    type = "qc_30k"
    build_path = "./build_tokenizers/with_lang_"${tokenizers.type}
    token_files = "./build_tokenizers/token_files_"${tokenizers.type}
}

embeddings {
    sbert {
        model="bert-base-nli-mean-tokens"
        pickle_path="./pickles"
    }
}

dataset {
    common_params {
        parallelize = false
        do_lowercase = true
        special_tokens = ["<unk>", "<lg>", "<qy>"]
        use_lang_weights = True
        code_max_num_tokens=1024
        use_subtokens = false # to do later
        #query_embeddings="sbert"
        fraction_using_func_name=0.1
        use_ast = "tree-sitter"
        ast_added_nodes = {
            "php": {"prefix": "<?php", "suffix": "?>"},
            "java": {"prefix": "class Toto {", "suffix": "}"}
        }
        ast_skip_node_types = {"php": ["ERROR", "<?php", "?>"], "java": ["ERROR"]}

        ast_special_tokens_files = [ "./pickles/test_special_tokens.json" ]
    }
}

training {
    short_circuit = true

    device = "cuda"
    wandb = false
    tensorboard = false

    name = "qc_ast"
    iteration = "2020_03_15"
    tokenizer_type = ${tokenizers.type}"_ast"

    model {
        training_ctx_class = "codenets.codesearchnet.query_code_siamese.training_ctx.QueryCodeSiameseCtx"
        encoder {
            hidden_size = 64
            vocab_size = ${common_vocab_size}
            intermediate_size = 512
            num_hidden_layers = 3
            num_attention_heads = 8
        }
    }
    lr = 0.00001

    loss {
        type = "softmax_cross_entropy"
    }

    batch_size {
        train = 8
        val = 8
        test = 8
        #train = 5
        #val = 5
        #test = 5
    }

}